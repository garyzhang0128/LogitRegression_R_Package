---
title: "R Notebook"
output: html_notebook
---

 
```{r}
setwd("D:/Universiteit/3-1/数据分析语言基础/pkg")
```


```{r}
df = read.csv(
  'breast-cancer-wisconsin.data',
  header = FALSE,
  col.names = c(
    "Sample code number",
    "Clump Thickness",
    "Uniformity of Cell Size",
    "Uniformity of Cell Shape",
    "Marginal Adhesion",
    "Single Epithelial Cell Size",
    "Bare Nuclei",
    "Bland Chromatin",
    "Normal Nucleoli",
    "Mitoses",
    "Class"
  ),
  na.strings = "?"
)

df$Bare.Nuclei[is.na(df$Bare.Nuclei)] = median(df$Bare.Nuclei, na.rm = T)

class_trans <- function(x) {
  if (x == 2) {
    return(1)
  }
  return(0)
}
df['Class'] = apply(df['Class'], MARGIN = 1, FUN = class_trans)

save(df, file = "breast_cancer_wisconsin.rda")
```
```{r}
load('breast_cancer_wisconsin.rda')
df
```
```{r}
X = df[, 2:10]
X = as.matrix(X)
X = cbind(X, c(1))
colnames(X)[10] = "intercept"
dim(X)
head(X)
```
```{r}
y = df[11]
dim(y)
```

```{r}
logit <-
  glm(
    Class ~ Clump.Thickness +
      Marginal.Adhesion + Bare.Nuclei + Bland.Chromatin +
      Mitoses,
    family = binomial(link = "logit"),
    data = df
  )
summary(logit)
```


```{r}
lossFunction <- function(X,  w, y, L2 = FALSE, alpha = 1) {
  loss_vector = -X %*% w * y + log(1 + exp(X %*% w))
  loss_sum = sum(loss_vector)
  if (L2) {
    return((loss_sum + alpha * t(w) %*% w)[1, 1])
  }
  else{
    return(loss_sum)
  }
}

gradient <- function(X, w, y, L2 = FALSE, alpha = 1) {
  grad = t(X) %*% as.matrix(1 / (1 + exp(-X %*% w)) - y)
  if (L2) {
    return(grad + alpha * 2 * w)
  }
  else{
    return(grad)
  }
  
}

hessian <- function(X, w, L2 = FALSE, alpha = 1) {
  n = dim(X)[1]
  S = matrix(0, n, n)
  
  for (i in 1:n) {
    miu_i = 1 / (1 + exp(-X %*% w))[i]
    S[i, i] = miu_i * (1 - miu_i)
  }
  
  if (L2) {
    return(t(X) %*% S %*% X + 2 * alpha)
  }
  else{
    return(t(X) %*% S %*% X)
  }
}

prox <- function(X,
                 v,
                 y,
                 L2 = FALSE,
                 alpha = 1,
                 lamda = 0.01) {
  grad = gradient(X, v, y, L2, alpha)
  return(v - lamda * grad)
}
```

```{r}
gradDesc <-
  function(X,
           w,
           y,
           L2 = FALSE,
           alpha = 1,
           iter = 100000,
           lamda = 0.0001,
           eps = 0.001) {
    for (i in 1:iter) {
      loss = lossFunction(X, w, y, L2, alpha)
      grad = gradient(X, w, y, L2, alpha)
      if (sqrt(sum(grad ^ 2)) < eps) {
        print(paste("iteration:", i))
        return(w)
      }
      w = w - lamda * grad
    }
    print(paste("iteration:", i))
    return(w)
  }
```
```{r}
library(MASS)
newton <- function(X,
                   w,
                   y,
                   L2 = FALSE,
                   alpha = 1,
                   iter = 1000,
                   lamda = 0.1,
                   eps = 0.001) {
  for (i in 1:iter) {
    loss = lossFunction(X, w, y, L2, alpha)
    grad = gradient(X, w, y, L2, alpha)
    hess = hessian(X, w, L2, alpha)
    if (sqrt(sum(grad ^ 2)) < eps) {
      print(paste("iteration:", i))
      return(w)
    }
    w = w - ginv(hess) %*% grad * lamda
  }
  print(paste("iteration:", i))
  return(w)
}
```
```{r}
nesGrad <-
  function(X,
           w,
           y,
           L2 = FALSE,
           alpha = 1,
           iter = 1000000,
           lamda = 0.0001,
           eps = 0.001) {
    i = 1
    v = w
    w_ori = w
    w = prox(X,v,y,L2,alpha,lamda)
    for (i in 2:iter) {
      loss = lossFunction(X, w, y, L2, alpha)
      grad = gradient(X, w, y, L2, alpha)
      if (sqrt(sum(grad ^ 2)) < eps) {
        print(paste("iteration:", i))
        return(w)
      }
      v = w + (i - 2) / (i + 1)*(w - w_ori)
      w_ori = w
      w = prox(X,v,y,L2,alpha,lamda)
    }
    print(paste("iteration:", i))
    return(w)
  }
```

```{r}
set.seed(1)
w = as.matrix(rnorm(10))
w
```


```{r}
gradDesc(X,w,y)
```

```{r}
gradDesc(X,w,y,L2=TRUE)
```

```{r}
newton(X,w,y)
```
```{r}
newton(X,w,y,L2=TRUE)
```


```{r}
nesGrad(X,w,y,lamda=0.0001)
```
```{r}
nesGrad(X,w,y,L2=TRUE)
```

```{r}
predict <- function(X, w, threshold = 0.5) {
  X = cbind(X, c(1))
  p_1 = 1 / (1 + exp(-X %*% w))
  prediction = cbind(p_1, c(0))
  for (i in 1:dim(p_1)[1]) {
    if (p_1[i, 1] > 0.5) {
      prediction[i, 2] = 1
    }
  }
  colnames(prediction) = c("P(y=1|w)", "Class_Predicted")
  return(prediction)
}
```

```{r}
predict(X[1:10,-10],w)
```
